---
title: "Homework 4"
author: "Yue Cao"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The ridge regression vignette}
-->

##1
Exercise 5.8, Question 2.

###Solution:
From equation (5.19), we know that the Hessian matrix is as follows:
$$
H(l)=X^t \cdot diag(p \cdot (1-p)) \cdot X
$$
An ill-conditioned matrix is a matrix whose condition number is large, while a well-conditioned matrix is a matrix whose condition number is small. Condition number is the ratio of the largest to smallest singular value in the singular value decomposition of a matrix. Suppose that the singular value decomposition of the matrix $X^tX$ is as follows:
$$
X^tX = U \Sigma V^t
$$
where
$U$ is an $n \times n$ unitary matrix,
$\Sigma$ is a diagonal $n \times p$ matrix with non-negative real numbers on the diagonal,
$V$ is an $p \times p$ unitary matrix, and $V^t$ is the transpose of $V$.

Then for the the Hessian matrix in (5.19), its singular value decomposition is
$$
H(l)=X^t \cdot diag(p \cdot (1-p)) \cdot X = U \tilde\Sigma V^t
$$
where $\tilde \Sigma = diag(p \cdot (1-p)) \cdot \Sigma$ and $U$, $V$ as before.

To be specific, if $n \ge p$, we assume that $\Sigma = diag(\sigma_{1}, \sigma_2, ..., \sigma_p)$. Therefore, $\tilde \Sigma = diag(p_1(1-p_1)\sigma_1, \quad p_2(1-p_2)\sigma_2, ..., p_p(1-p_p)\sigma_p)$;

If $n<p$, we assume that $\Sigma = diag(\sigma_{1}, \sigma_2, ..., \sigma_n)$. Therefore, $\tilde \Sigma = diag(p_1(1-p_1)\sigma_1, p_2(1-p_2)\sigma_2, ..., p_n(1-p_n)\sigma_n)$.

*Here subscript $p$ stands for the dimension of $\beta$, and normal $p_i$ stands for probabilities, sorry for a little bit confusing

To make this Hessian matrix more ill-conditioned than the matrix $X^tX$ itself, we need to make sure that the condition number of the Hessian matrix $H(l)$ is larger than that of the matrix $X^tX$ itself. That means that if we know the specific position of the largest singular values in the $\Sigma$, lets suppose that it is $k$ ($1\le k \le p$ if $n \ge p$ or $1\le k \le n$ if $n < p$). We can keep $k$ as the largest singular values position in $\tilde\Sigma$ by letting $p_k(1-p_k)$ as large as it can be.It is easy to know that when $p=0.5$, $p(1-p)$ can reach it maximum. So that we will set $p_k = 0.5$, now $p_k(1-p_k)\sigma_k$ must be the largest singular value of the Hessian matrix $H(l)$.

For the smallest singular value of $H(l)$, we just need to guarantee that one of the singular values of $H(l)$ is infinitely approaching $0$. That is to say, we hope that for $1\le i \le p$ (if $n \ge p$) or $1\le i \le n$ (if $n < p$), $p_i(1-p_i)$ is pretty close to $0$, which indicates that $p_i(1-p_i)\sigma_i$ must be the smallest singular value of $H(l)$ approaching infinitely to $0$. In this case, $p_i$ will be very close to $0$ or $1$, such as $0.0000001$ or $0.999999$.

And with these two singular values (the largest and the smallest one), we make sure that the condition number (ratio of the largest singular value to the smallest singular value) of the Hessian matrix approaches infinity, and must be bigger than that of the matrix $X^tX$.

It is unnecessary to constrain the matrix $X^tX$, just avoiding that it is singular (or its condition number will be infinite).

And we can generate a simple $2 \times 2$ matrix $X$ as follows:
```{r}
X <- matrix(c(1, 2, 3.5, 1), nrow = 2, ncol = 2)
XtX <- crossprod(X, X)
D <- svd(XtX)$d
cn <- D[1]/D[2]
# The condition number of XtX is 7.111111
XpX <- crossprod(X, diag(c(0.5*(1-0.5), 0.0000001*(1-0.0000001))) %*% X)
D2 <- svd(XpX)$d
cn2 <- D2[1]/D2[2]
# The condition number of the Hessian matrix is 12191843
```

In this case, we also let $p_1=0.5, p_2=0.0000001$, we can see that the linear Hessian $(X^tX)$ is well-conditioned but the logistic variation is not.



##2
Exercise 5.8, Question 4.

###Solution:
Just to simplify our discuss, we will start with a canonical exponential family.

In this case, the likelihood function is as follows:
$$
\mathcal{L}(y)=\prod h(y_i) \cdot exp \{x_i^t\beta \cdot y_i - A(x_i^t\beta)\}
$$
The log likelihood is then,
$$
l(y) = \sum_{i=1}^n x_i^t\beta \cdot y_i - A(x_i^t\beta) + log(h(y_i))
$$

Add a ridge penalty to the likelihood function, we have
$$
l(y) = \sum_{i=1}^n x_i^t\beta \cdot y_i - A(x_i^t\beta) + log(h(y_i)) - \frac{1}{2}\lambda\Vert\beta\Vert^2
$$
With first derivatives given by
$$
\frac{\partial l}{\partial \beta_j}=\sum_{i=1}^nx_{i,j} \cdot [y_i - A'(x_i^t\beta)] - \lambda\beta_j
$$
And the second derivatives derived similarly as
$$
\frac{\partial^2 l}{\partial \beta_k \partial \beta_j}=\sum_{i=1}^n x_{i,j}x_{i,k} \cdot [ - A''(x_i^t\beta)] - \lambda \cdot \mathbf{1}(k=j)
$$
where $\mathbf{1}(k=j)$ is an indicator function.

In the canonical form, the first derivative of $A$ with respect to $\eta$ gives the expected value of the random variable and the second derivative of $A$ with respect to $\eta$ yields the variance. (We can slightly update them in Iteratively Re-Weighted Least Squares later.) Therefore, the gradient function is
$$
\nabla_\beta l = X^t(y-\mathbb{E}y) - \lambda\beta,
$$
and the Hessian matrix becomes
$$ 
H = X^t \cdot diag(Var(y)) \cdot X + \lambda I_p
$$
Using the Newton-Raphson method with our formulae for for the Hessian and the gradient functions
$$
\begin{aligned}
\beta^{(k+1)} &= \beta^{(k)} - H^{-1}(l)(\beta^{(k)}) \cdot \nabla_\beta (l)\beta^{(k)} \\
&= \beta^{(k)} + \left[X^t \cdot diag(Var(y)) \cdot X + \lambda I_p\right]^{-1} \cdot \left[X^t(y-\mathbb{E}y) - \lambda\beta^{(k)}\right]
\end{aligned}
$$
Defining $W$ as a diagnonal matrix of the variances ($W=diag(Var(y))$), this simplifies to
$$
\begin{aligned}
\beta^{(k+1)}
&= \beta^{(k)} + \left[X^t W X + \lambda I_p\right]^{-1} \cdot \left[X^t(y-\mathbb{E}y) - \lambda\beta^{(k)}\right] \\
&= \left[X^t W X + \lambda I_p\right]^{-1}\left[\left(X^t W^t X + \lambda I_p\right)\beta^{(k)}-\lambda \beta^{(k)}+(X^tW^t) \cdot \left(\frac{y-\mathbb{E}^{(k)}}{W}\right)\right] \\
&= \left[X^t W X + \lambda I_p\right]^{-1}\left[\left(X^t W^t X + \lambda I_p-\lambda I_p\right)\beta^{(k)}+(X^tW^t) \cdot \left(\frac{y-\mathbb{E}^{(k)}}{W}\right)\right] \\
&= \left[X^t W X + \lambda I_p\right]^{-1} \cdot (X^tW^t) \cdot \left\{X\beta^{(k)} + \left(\frac{y-\mathbb{E}y^{(k)}}{W}\right)\right\} \\
&= \left[X^t W X + \lambda I_p\right]^{-1}X^tW^tz
\end{aligned}
$$
where $z=\left\{X\beta^{(k)} + \left(\frac{y-\mathbb{E}y^{(k)}}{W}\right)\right\}$.
For Iteratively Re-Weighted Least Squares (IRWLS), we just need to slightly update $W$ and $z$ as follows:
$$
W = diag(\mu'(X\beta)^2/Var(y^{(k)})),
$$
and
$$
z=X\beta + \frac{y-\mu(X\beta)}{\mu'(X\beta)}.
$$
Thus, we can modify the function \texttt{irwls_glm} as follows:
```{r}
casl_glm_irwls_ridge <- function(X, y, family, lambda, maxit=25, tol=1e-10)
{
  beta <- rep(0, ncol(X))
  for (j in seq_len(maxit))
  {
    b_old <- beta
    eta <- X %*% beta
    mu <- family$linkinv(eta)
    mu_p <- family$mu.eta(eta)
    z <- eta + (y - mu) / mu_p
    W <- as.numeric(mu_p^2 / family$variance(mu))
    XtX <- crossprod(X, diag(W) %*% X) + diag(rep(lambda, ncol(X)))
    Xtz <- crossprod(X, W * z)
    beta <- solve(XtX, Xtz)
    if(sqrt(crossprod(beta - b_old)) < tol) break
  }
  beta
}
```



##3
Consider the sparse matrix implementation from class and the sparse add
function:
```{r}
sparse_add <- function(a, b) {
  c <- merge(a, b, by = c("i", "j"), all = TRUE, suffixes = c("1", "2"))
  c$x1[is.na(c$x1)] <- 0
  c$x2[is.na(c$x2)] <- 0
  c$x <- c$x1 + c$x2
  c[, c("i", "j", "x")]
}

a <- data.frame(i = c(1, 2), j = c(1, 1), x = c(3, 1))
b <- data.frame(i = c(1, 2, 2), j = c(1, 1, 2), x = c(4.4, 1.2, 3))
sparse_add(a, b)
```

        - Implement a `sparse_multiply` function that multiplies two sparse matrices.
        - Create a new class `sparse.matrix` that has add `+`, multiply `%*%`, and transpose `t()` methods.
        - Add test-sparse-matrix.r to the testthat directory of your bis557 package to show it works.

First, implement a 'sparse_multiply' function that multiplies two sparse matrices. Here, I also implement a 'sparse_transpose' function to transpose a matrix. In this way, we can simplify the algorithm of sparse matrix multiplication.
```{r}
sparse_transpose <- function(a) {
  at <- a
  at$i <- a$j
  at$j <- a$i
  at$x <- a$x
  at <- at[order(at$i, at$j),]
  at[, c("i", "j", "x")]
}

sparse_multiply <- function(a, b) {
  bt <- sparse_transpose(b)
  cc <- merge(a, bt, by = c("j"), all = TRUE, suffixes = c("1", "2"))
  cc$x <- cc$x1 * cc$x2
  cc <- na.omit(cc)
  cc$group <- cc %>% group_by(i1, i2) %>% group_indices()
  c <- unique(cc[, c("i1", "i2")])
  c$x <- rowsum(cc$x, cc$group)
  #c <- aggregate(cc$x, by=list(cc$i1, cc$i2), FUN=sum)
  names(c) <- c("i", "j", "x")
  c <- c[order(c$i, c$j),]
  c[, c("i", "j", "x")]
}

a <- data.frame(i = c(1, 2), j = c(1, 1), x = c(3, 1))
b <- data.frame(i = c(1, 2, 2), j = c(1, 1, 2), x = c(4.4, 1.2, 3))
sparse_multiply(a, b)
```

Then, we are gonna create a new class `sparse.matrix` that has add `+`, multiply `%*%`, and transpose `t()` methods as follows:
```{r}
library(dplyr)

sparse.matrix <- function(i, j, x, dims) {
  if (missing(dims)) {
    a <- data.frame(i = i, j = j , x = x)
    attr(a, "dims") <- c(max(i), max(j))
  } 
  else {
    a<- data.frame(i = i, j = j, x = x)
    attr(a, "dims") <- dims
  }
  class(a) <- c("sparse.matrix", class(a))
  a
}

'+.sparse.matrix' <- function(a, b) {
  if (attr(a, "dims")[1] != attr(b, "dims")[1] || attr(a, "dims")[2] != attr(b, "dims")[2]) {
    stop("should not add these two matrices.")
  }
  c <- merge(a, b, by = c("i", "j"), all = TRUE, suffixes = c("1", "2"))
  c$x1[is.na(c$x1)] <- 0
  c$x2[is.na(c$x2)] <- 0
  c$x <- c$x1 + c$x2
  c <- c[order(c$j),]
  attr(c, "dims") <- attr(a, "dims")
  sparse.matrix(c$i, c$j, c$x, attr(c, "dims"))
}

't.sparse.matrix' <- function(a) {
  at <- a
  at$i <- a$j
  at$j <- a$i
  at$x <- a$x
  at <- at[order(at$i, at$j),]
  attr(at, "dims")[1] <- attr(a, "dims")[2]
  attr(at, "dims")[2] <- attr(a, "dims")[1]
  sparse.matrix(at$i, at$j, at$x, attr(at, "dims")) 
}

'%*%' <- function(a, b) {
  UseMethod("%*%", a)
}

'%*%.default' <- function(a, b) {
  warning("No method to dispatch to, using default.")
  a %*% b
}

'%*%.matrix' <- function(a, b) {
  if (!inherits(b, "matrix") && !inherits(b, "Matrix")) {
    stop("b argument is not a matrix type.")
  }
  a %*% b
}

'%*%.numeric' <- function(a, b) {
  if (!inherits(y, "numeric")) {
    stop("b argunment is not a numeric vector type")
  }
  a %*% b
}

'%*%.sparse.matrix' <- function(a, b) {
  if (attr(a, "dims")[2] != attr(b, "dims")[1]) {
    stop("should not multiply these two matrices.")
  }
  
  cc <- merge(a, t(b), by = c("j"), all = TRUE, suffixes = c("1", "2"))
  cc <- na.omit(cc)
  cc$x <- cc$x1 * cc$x2
  
  #cc <- subset(cc, select = c("i1", "i2", "x"))
  #cc$group <- cc %>% group_by(i1, i2) %>% group_indices()
  #c <- unique(cc[, c("i1", "i2")])
  #c$x <- rowsum(cc$x, cc$group)
  
  c <- aggregate(cc$x, by=list(cc$i1, cc$i2), FUN=sum)
  
  #c <- cc %>%
  #  group_by(i1, i2) %>%
  #  summarise(x=sum(x))
  
  names(c)[1:2] <- c("i", "j")
  c <- c[order(c$j),]
  attr(c, "dims")[1] <- attr(a, "dims")[1]
  attr(c, "dims")[2] <- attr(b, "dims")[2]
  dims <- attr(c, "dims")
  sparse.matrix(c$i, c$j, c$x, dims)
}
```