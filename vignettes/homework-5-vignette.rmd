---
title: "Homework 5"
author: "Yue Cao"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The homework 5 vignette}
-->

##Question 1. We want to use all lambda values in cv.glmnet to evaluate the highest out-of-sample accuracy.
```{r}
library(keras)
library(glmnet)
install_keras()

# The data, shuffled and split between train and test sets
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(60000, 28^2))
x_test <- array_reshape(x_test, c(10000, 28^2))
y_train <- factor(y_train)
y_test <- factor(y_test)

set.seed(423)
s <- sample(seq_along(y_train), 1000)
fit <- cv.glmnet(x_train[s,], y_train[s], family = "multinomial")

preds <- predict(fit$glmnet.fit, x_train[s,], s = fit$lambda.min, type = "class")

table(as.vector(preds), y_train[s])

len <- length(fit$lambda)
accuracy <- rep(0, len)
for (i in 1:len) {
  preds <- predict(fit$glmnet.fit, x_test, s = fit$lambda[i], type = "class")
  t <- table(as.vector(preds), y_test)
  accuracy[i] <- sum(diag(t)) / sum(t)
}

df <- data.frame("accuracy" = accuracy, "lambda" = fit$lambda)
df[which.max(df$accuracy), ]
```

We can see that if $\lambda=0.002236499$, the out-of-sample accuracy reaches its maximum in this case. Intuitively, it means that smaller $\lambda$ can give the lasso regression model more flexibility, letting more coefficients not equal to zero, which indicates extracting more preditive features from the images.

##2. (We check with Professor Kane, he said it is fine to use MNIST in this question.)
```{r}
# Transform RGB values into [0,1] range
x_train <- (x_train / 255)
x_test <- (x_test / 255)

# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes=26L)
y_test <- to_categorical(y_test, num_classes=26L)
X_train <- array_reshape(as.matrix(x_train), dim = c(nrow(x_train), 28, 28, 1))
X_valid <- array_reshape(as.matrix(x_test), dim = c(nrow(x_test), 28, 28, 1))
Y_train <- y_train
Y_valid <- y_test

model <- keras_model_sequential()
model %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2),
                input_shape = c(28, 28, 1), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(rate = 0.5) %>%
  
  layer_conv_2d(filters = 64, kernel_size = c(2,2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_conv_2d(filters = 64, kernel_size = c(2,2), padding = "same") %>%
  layer_activation(activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(rate = 0.5) %>% 
  
  layer_flatten() %>% 
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dense(units = 128) %>%
  layer_activation(activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 26) %>%
  layer_activation(activation = "softmax")

model %>% compile(loss = 'categorical_crossentropy',
                  optimizer = optimizer_rmsprop(),
                  metrics = c('accuracy'))

# Prediction for training and validation sets
train_predict <- predict_classes(model, X_train)
test_predict <- predict_classes(model, X_valid)

# Change binary outcomes back to original classes
y_original_train <- rep(NA, nrow(y_train))
for (j in 1:26) {
  for (i in 1:nrow(y_train)) {
    if (y_train[i,j] != 0) {
      y_original_train[i] <- j - 1
    }
  }
}

y_original_test <- rep(NA, nrow(y_test))
for (j in 1:26) {
  for (i in 1:nrow(y_test)) {
    if (y_test[i,j] != 0) {
      y_original_test[i] <- j - 1
    }
  }
}
# Prediction accuracy
sum(train_predict == y_original_train)/60000
sum(test_predict == y_original_test)/10000
```

Using MNIST dataset, we can see with the same conditions in section 8.10.4, the training accuracy is 0.98675 and the test accuracy is 0.9874.

Generally, when using a CNN, the four important hyperparameters we have to decide on are:
- the kernel size
- the filter count (that is, how many filters do we want to use)
- stride (how big are the steps of the filter)
- padding

Changing the kernel_size to c(4,4), the training accuracy decreases to 0.8984 and the test accuracy decreases to 0.8998.

Increasing the filters from 32 to 64, the training accuracy decreases a little bit to 0.9712 and the test accuracy decreases a little bit to 0.9683.


##3.
```{r}
library(ggplot2)

# Create list of weights to describe a dense neural network.
#
# Args:
#     sizes: A vector giving the size of each layer, including
#            the input and output layers.
#
# Returns:
#     A list containing initialized weights and biases.
casl_nn_make_weights <-
function(sizes)
{
  L <- length(sizes) - 1L
  weights <- vector("list", L)
  for (j in seq_len(L))
  {
    w <- matrix(rnorm(sizes[j] * sizes[j + 1L]),
                ncol = sizes[j],
                nrow = sizes[j + 1L])
    weights[[j]] <- list(w=w, 
                         b=rnorm(sizes[j + 1L]))
  }
  weights 
}

# Apply a rectified linear unit (ReLU) to a vector/matrix.
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     The original input with negative values truncated to zero.
casl_util_ReLU <-
function(v)
{
  v[v < 0] <- 0
  v
}

# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     v: A numeric vector or matrix.
#
# Returns:
#     Sets positive values to 1 and negative values to zero.
casl_util_ReLU_p <-
function(v)
{
  p <- v * 0
  p[v > 0] <- 1
  p
}

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Derivative of the mean absolute deviation function
#
# Args:
#     y: A numeric vector of responses.
#     a: A numeric vector of predicted responses.
#     m: A function to calculate a central point: mean, median or getmode
#
#Returns:
#     Returned current derivative of the mean absolute deviation function
casl_util_mad_p <- function(y, a, m)
{
  ab <- m(a) - y #m(a)
  ab[ab < 0] <- -1
  ab[ab >= 0 ] <- 1
  ab
}

# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     weights: A list created by casl_nn_make_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_forward_prop <-
function(x, weights, sigma)
{
L <- length(weights)
z <- vector("list", L)
a <- vector("list", L)
for (j in seq_len(L))
{
  a_j1 <- if(j == 1) x else a[[j - 1L]]
  z[[j]] <- weights[[j]]$w %*% a_j1 + weights[[j]]$b
  a[[j]] <- if (j != L) sigma(z[[j]]) else z[[j]]
  }
  list(z=z, a=a)
}

# Apply backward propagation algorithm.
#
# Args:
#     x: A numeric vector representing one row of the input.
#     y: A numeric vector representing one row of the response.
#     weights: A list created by casl_nn_make_weights.
#     f_obj: Output of the function casl_nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_backward_prop <-
function(x, y, weights, f_obj, sigma_p, f_p)
{
  z <- f_obj$z; a <- f_obj$a
  L <- length(weights)
  grad_z <- vector("list", L)
  grad_w <- vector("list", L)
  for (j in rev(seq_len(L)))
  {
    if (j == L) {
      grad_z[[j]] <- f_p(y, a[[j]], mean)
    } 
    else {
      grad_z[[j]] <- (t(weights[[j + 1]]$w) %*%
      grad_z[[j + 1]]) * sigma_p(z[[j]])
    }
    a_j1 <- if(j == 1) x else a[[j - 1L]]
    grad_w[[j]] <- grad_z[[j]] %*% t(a_j1)
  }
  list(grad_z=grad_z, grad_w=grad_w)
}

# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in
#            the neural network.
#     epochs: Integer number of epochs to computer.
#     eta: Positive numeric learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd <-
function(X, y, sizes, epochs, eta, weights=NULL)
{
  if (is.null(weights)) {
    weights <- casl_nn_make_weights(sizes)
  }
  for (epoch in seq_len(epochs)) {
    for (i in seq_len(nrow(X))) {
      f_obj <- casl_nn_forward_prop(X[i,], weights,
                                    casl_util_ReLU)
      b_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
                                     f_obj, casl_util_ReLU_p,
                                     casl_util_mad_p)
      for (j in seq_along(b_obj)) {
        weights[[j]]$b <- weights[[j]]$b - eta * b_obj$grad_z[[j]]
        weights[[j]]$w <- weights[[j]]$w - eta * b_obj$grad_w[[j]]
      }
    } 
  }
  weights
}

# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
casl_nn_predict <-
function(weights, X_test)
{
  p <- length(weights[[length(weights)]]$b)
  y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
  for (i in seq_len(nrow(X_test))) {
    a <- casl_nn_forward_prop(X_test[i,], weights,
                              casl_util_ReLU)$a
    y_hat[i, ] <- a[[length(a)]]
  }
  y_hat
}

X <- matrix(runif(1000, min=-1, max=1), ncol=1)
y <- X[,1,drop = FALSE]^2 + rnorm(1000, sd = 0.1)
s <- sample(1:1000, 50, replace = FALSE)
y[s] <- c(runif(25, min=4, max=8), runif(25, min=-8, max=-4))

weights <- casl_nn_sgd(X, y, sizes=c(1, 25, 1), epochs=25, eta=0.01)
y_pred <- casl_nn_predict(weights, X)
df <- data.frame("x" = X, "y" = y, "pred" = y_pred)

ggplot(df) + 
  geom_point(aes(x=x, y=y, color = "blue")) + 
  geom_point(aes(x=x, y=y_pred, color = "pink")) +
  theme(legend.position="none")
```

The red points stand for the original simulations with several outlines. The blue line is associated with the predictions. It seems that neural network and SGD perform well when outliners existing. They are not influenced by the outliners at all!


