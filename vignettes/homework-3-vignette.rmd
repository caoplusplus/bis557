---
title: "Homework 3"
author: "Yue Cao"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: yes
    toc: true
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{The ridge regression vignette}
-->

##1. Page 117, question 7.
```{r}
kern_density <- function(x, h, x_new) {
  kde <- c()
  for (i in 1:length(x_new)) {
    f <- mean(0.75*(1-((x_new[i] - x)/h)^2)*ifelse(abs((x_new[i] - x)/h) <= 1, 1, 0))/h
    kde <- c(kde, f)
  }
  return(kde)
}

x <- rnorm(1000)
x_new <- rnorm(200)
t <- dnorm(x_new)
# Bandwidth = 0.4
kde <- matrix(NA, nrow = 200, ncol = 8)
kde[, 1] <- x_new
kde[, 2] <- t
h <- seq(0.2, 1.2, by = 0.2)
for (i in 1:6) {
  kde[, i+2] <- kern_density(x, h[i], x_new)
}
kde <- as.data.frame(kde)
colnames(kde) <- c("x_new", "true_density", "kde1", "kde2", "kde3", "kde4", "kde5", "kde6")
kde <- kde[order(kde[, 1]), ]
  
par(mfrow=c(2, 3))
plot(x_new, t, cex = 0.1, col = "blue", ylab = "density",
     main = "bandwidth = 0.2")
lines(kde[, 1], kde[, 2], col = "blue")
lines(kde[, 1], kde[, 3], col = "pink")
plot(x_new, t, cex = 0.1, col = "blue", ylab = "density",
     main = "bandwidth = 0.4")
lines(kde[, 1], kde[, 2], col = "blue")
lines(kde[, 1], kde[, 4], col = "pink")
plot(x_new, t, cex = 0.1, col = "blue", ylab = "density",
     main = "bandwidth = 0.6")
lines(kde[, 1], kde[, 2], col = "blue")
lines(kde[, 1], kde[, 5], col = "pink")
plot(x_new, t, cex = 0.1, col = "blue", ylab = "density",
     main = "bandwidth = 0.8")
lines(kde[, 1], kde[, 2], col = "blue")
lines(kde[, 1], kde[, 6], col = "pink")
plot(x_new, t, cex = 0.1, col = "blue", ylab = "density",
     main = "bandwidth = 1.0")
lines(kde[, 1], kde[, 2], col = "blue")
lines(kde[, 1], kde[, 7], col = "pink")
plot(x_new, t, cex = 0.1, col = "blue", ylab = "density",
     main = "bandwidth = 1.2")
lines(kde[, 1], kde[, 2], col = "blue")
lines(kde[, 1], kde[, 8], col = "pink")

```


##2. Page 200, question 3.

###Proof:
Suppose that $f(x)$ and $g(x)$ are both convex functions on an interval $[a, b]$, we know that for any two points $x_1$ and $x_2$ in $[a, b]$ and any $\lambda$ where $0<\lambda<1$, we have:
$$
f[\lambda x_1 + (1-\lambda)x_2] \le \lambda f(x_1) + (1-\lambda)f(x_2)
$$
and
$$
g[\lambda x_1 + (1-\lambda)x_2] \le \lambda g(x_1) + (1-\lambda)g(x_2)
$$
Therefore, let $h=f+g$, for any two points $x_1$ and $x_2$ in $[a, b]$ and any $\lambda$ where $0<\lambda<1$,
$$
\begin{aligned}
h[\lambda x_1 + (1-\lambda)x_2] &= f[\lambda x_1 + (1-\lambda)x_2]+g[\lambda x_1 + (1-\lambda)x_2] \\
&\le \lambda(f(x_1) + g(x_1)) + (1-\lambda)(f(x_2) + g(x_2)) \\
&= \lambda h(x_1) + (1-\lambda)h(x_2),
\end{aligned}
$$
which means $h$ is a convex function.

So that we prove if $f$ and $g$ are both convex functions, then their sum must also be convex.


##3. Page 200, question 5.

###Proof
Let $x_1, x_2 \ge 0 \in \mathbb{R}$, for any $\lambda$ where $0<\lambda<1$:
$$
\begin{aligned}
f(\lambda x_1 + (1 - \lambda)x_2) &= |\lambda x_1 + (1 - \lambda)x_2| \\
&\le |\lambda x_1| + |(1 - \lambda)x_2| \\
&= |\lambda||x_1| + |(1 - \lambda)||x_2| \\
&= \lambda|x_1| + (1 - \lambda)|x_2| \\
&= \lambda f(x_1) + (1 - \lambda)f(x_2)
\end{aligned}
$$
According to the definition of a convex function, we are easy to know that absolute value function is convex.

The $\ell_1$-norm of a vector $\boldsymbol{x}$ is as follows:
$$
\Vert \boldsymbol{x}\Vert_1=\sum_{i=1}^n|x_i|
$$
which is the sum of $n$ absolute value functions. Since we have proved that absolute value function is convex and the sum of convex functions is convex, the $\ell_1$-norm is also convex.


##4. Page 200, question 5.

###Proof
The elastic net objective function is as follows:
$$
f(b;\lambda, \alpha)=\frac{1}{2n}\Vert y-Xb \Vert_2^2 + \lambda \left((1 - \alpha)\frac{1}{2}\Vert b \Vert_2^2 + \alpha \Vert b \Vert_1 \right)
$$
where $\lambda > 0$ and $\alpha \in [0, 1]$.

From the previous exercise, we know that $\Vert b \Vert_1$ is convex. Now we need to prove that both $g(b)=(y_i-X_ib)^2$ and $h(b_j)=b_j^2$ are convex, so that the sum of these three functions is convex, which is the elastic net objective function.

For $h(b_j)=b_j^2$, we have
$$
h'(b_j)=2b_j
$$
and
$$
h''(b_j)=2 \ge 0
$$
so that $h(b_j)=b_j^2$ is convex. Since $\Vert b \Vert_2^2$ is the sum of all $b_j^2$, from question 3 we know that $\Vert b \Vert_2^2$ is convex. Thus, $\lambda \left((1 - \alpha)\frac{1}{2}\Vert b \Vert_2^2 + \alpha \Vert b \Vert_1 \right)$ is convex.

Next we are gonna prove that $g(b)=(y_i-X_ib)^2$ is convex. For any values $b_1, b_2 \in \mathbb{R}^p$ and any $\lambda$ where $0<\lambda<1$,
$$
\begin{aligned}
g(\lambda b_1 + (1 - \lambda)b_2) &= (y_i-X_i(\lambda b_1 + (1 - \lambda)b_2))^2 \\
&= y_i^2-2y_iX_i(\lambda b_1 + (1 - \lambda)b_2) + X_i^2(\lambda b_1 + (1 - \lambda)b_2)^2 \\
&= \lambda y_i + (1 - \lambda)y_i - 2\lambda b_1 y_iX_i - 2(1 - \lambda)b_2y_iX_i + \lambda^2 b_1^2 X_i^2 \\
&\quad + (1 - \lambda)^2b_2^2X_i^2 + 2\lambda(1-\lambda)b_1b_2X_i^2 \\
&= \lambda(y_i-X_ib_1)^2 + (1 - \lambda) (y_i-X_ib_2)^2 + \lambda^2b_1^2X_i^2 - \lambda b_1^2X_i^2 \\
&\quad + (1 - \lambda)^2b_2^2X_i^2 - (1 - \lambda)b_2^2X_i^2 + 2\lambda(1-\lambda)b_1b_2X_i^2 \\
&=  \lambda(y_i-X_ib_1)^2 + (1 - \lambda) (y_i-X_ib_2)^2 \\
&\quad + \left[-\lambda(1 - \lambda)b_1^2 - \lambda(1 - \lambda)b_2^2  + 2\lambda(1-\lambda)b_1b_2 \right]X_i^2 \\
&= \lambda(y_i-X_ib_1)^2 + (1 - \lambda) (y_i-X_ib_2)^2 - \lambda(1-\lambda)(b_1^2 + b_2^2-2b_1b_2)X_i^2 \\
&= \lambda(y_i-X_ib_1)^2 + (1 - \lambda) (y_i-X_ib_2)^2 - \lambda(1-\lambda)(b_1 - b_2)^2X_i^2
\end{aligned}
$$
Since $\lambda(1 - \lambda)(b_1-b_2)^2X_i^2 \ge 0$, then
$$
\begin{aligned}
g(\lambda b_1 + (1 - \lambda)b_2) &= \lambda(y_i-X_ib_1)^2 + (1 - \lambda) (y_i-X_ib_2)^2 - \lambda(1-\lambda)(b_1 - b_2)^2X_i^2 \\
& \le \lambda g(b_1) + (1 - \lambda)g(b_2)
\end{aligned}
$$
That $g(b)=(y_i-X_ib)^2$ is convex has been proven. Based on exercise 4, we believe that the elastic net objective function is convex.


##5. Page 200, question 6.

###Solution:
When $\alpha=1$, it is a lasso problem, given response $y \in \mathbb{R}^n$, predictors matrix $X \in \mathbb{R}^{n \times p}$, solve
$$
\min_{b \in \mathbb{R}^p}\frac{1}{2}\Vert y - Xb \Vert + \lambda\Vert b \Vert_1
$$
Let
$$
f(b)=\frac{1}{2n}\sum_{i=1}^n\left(y_i - \sum_{j=1}^p x_{ij}b_j \right)^2 + \lambda \sum_{j=1}^p|b_j|
$$
we have the derivative of the above function with respect to $b_l$ at $b=\tilde b$ is
$$
\frac{\partial{f}}{\partial b_l}|_{b=\tilde b}=-\frac{1}{n}\sum_{i=1}^nx_il\left(y_i - \sum_{j=1}^px_{ij}\tilde b_j \right)+\lambda \frac{\partial \Vert \tilde b \Vert_1}{\partial b_l}
$$
Then, the KKT conditions would be (denote $\hat b$ as the optimal fitted values of $b$) that:
$$
\sum_{i=1}^n x_{il}\left(y_i - \sum_{j=1}^p x_{ij} \hat b_j \right)=\lambda s_j
$$
for $j = 1, 2, ..., p$ where
$$
s_j \in
\begin{cases}
    1 & if \quad \hat b_j>0 \\
    -1 & if \quad \hat b_j<0 \\
    [-1, 1] & if \quad \hat b_j=0
\end{cases}
$$
```{r}
lasso_reg_with_screening <- function(X, y, lambda) {
  fit <- glmnet(X, y, alpha = 1, lambda = lambda)
  b <- as.vector(fit$beta)
  resids <- y - X %*% b
  s <- apply(X, 2, function(xj) crossprod(xj, resids)) / lambda / nrow(X)
  (b == 0) & (abs(s) >= 1)
}

X <- as.matrix(iris[, 2:4])
y <- iris$Sepal.Length
lasso_reg_with_screening(X, y, 0.3)
```

Function \textt{glmnet} doesn't satisfy the KKT conditions in the above case.






